{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKLn4aiWhk2h7GlOnDzsIB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorrespz/NLP-text-analysis/blob/main/Transformers_Pytorch_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XzPV7-hQF46A"
      },
      "outputs": [],
      "source": [
        "# Lazy Programmer's Transformers course\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multihead Attention Block"
      ],
      "metadata": {
        "id": "OVcb6NQcTLxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the formula:\n",
        "\n",
        "   Attention($Q, K, D$) = softmax$\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
        "\n",
        "  where:\n",
        "\n",
        "   $ Q = W^Q Q_{input}$\n",
        "\n",
        "   $ K = W^K K_{input}$\n",
        "\n",
        "   $V = W^V V_{input}$\n"
      ],
      "metadata": {
        "id": "d7MpXEkgJGNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d, k, d_model, n_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    #Assume d_v = d_k (len(Q) = len(K) = d_k, len(V) = d_v)\n",
        "    self.d_k = d_k\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    self.key = nn.Linear(d_model, d_k*n_heads)\n",
        "    self.query = nn.Linear(d_model, d_k*n_heads)\n",
        "    self.value = nn.Linear(d_model, d_k*n_heads)\n",
        "\n",
        "    #final linear layer\n",
        "    self.fc = nn.Linear(d_k*n_heads, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask = None):\n",
        "      q = self.query(q)   # N x T x (hd_k)\n",
        "      k = self.key(k)     # N x T x (hd_k)\n",
        "      v = self.value(v)     # N x T x (hd_v)\n",
        "      #h = n_heads\n",
        "\n",
        "      # N = batch size\n",
        "      N = q.shape[0]\n",
        "      # T = sequence length\n",
        "      T = q.shape[1]\n",
        "\n",
        "      #change the shape to:\n",
        "      # (N, T, h, d_k) --> (N, h, T, d_k)\n",
        "      q = q.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
        "      k = k.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
        "      v = v.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "      #compute attention weights\n",
        "      # q * k^T\n",
        "      #(N,  h, T,  d_k) x (N, h, d_k, T) --> (N, h, T, T)\n",
        "      #transposing the last 2 dimensions of k\n",
        "      attn_scores = q @ k.transpose(-2, -1)/math.sqrt(self.d_k)\n",
        "      #apply the mask, which is a tensor of size (N,T) of values 0, 1\n",
        "      #for each of the N samples, need to know which of the T tokens is important\n",
        "      #Change from 2D to 4D by adding None, which introduces superfluous dim of size 1\n",
        "      # (N, T) --> (N, 1, 1, T)\n",
        "      if mask is not None:\n",
        "        #mask_fill(arg1, arg2): if arg1 = True, apply arg2\n",
        "        #softmax(-inf) = 0\n",
        "        attn_scores = attn_scores.masked_fill(mask[:, None, None,:] == 0, float('-inf'))\n",
        "      attn_weights = F.softmax(attn_scores, dim = -1)\n",
        "\n",
        "      #compute attention-weighted values\n",
        "      #(N, h, T, T) x (N, h, T, d_k) --> (N, h, T, d_k)\n",
        "      A = attn_weights @ v\n",
        "\n",
        "      #reshape it back before the final linear layer\n",
        "      A = A.transpose(1, 2) # (N, T, h, d_k)\n",
        "      A = A.contiguous().view(N, T, self.d_k*self.n_heads) #(N, T, h*d_k)\n",
        "\n",
        "      #final step is to project A with the Linear layer to\n",
        "      #get the same shape as the input sequence\n",
        "      return self.fc(A)\n"
      ],
      "metadata": {
        "id": "XHHu-OGyGQ49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Block"
      ],
      "metadata": {
        "id": "cMXw2AfnJFWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        ""
      ],
      "metadata": {
        "id": "dTQPwC3lTFvY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}